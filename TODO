add a convolutional layer?
expanded representation - 5 nodes per point-player
depth_experiment - try with hidden_layer_sizes 1000, 1000, 1000 (say) and relu activation
method is less general than TD-lambda, but smoothens training (lookup table would converge without needing to lower learning rate); could replace with other tree searches cf alphazero; replay memory smoothening and separation of concerns/parallelisability cf atari; hybrid solves issue of very long games at the start
add an epsilon chance of playing randomly (and possiblye using an old version for smoothness) - do this some proportion of the time
for gammons and backgammons, pass an Outcome.t and a Player.t to equities and a (payoff : Outcome.t -> Player.t -> float) to games
give the network 5 output nodes (or possibly 3 if you want to ignore backgammons)
for a gammon pip count ratio, use pip count to point 6 plus 1 if nothing borne off, divided by sum of that plus opponent ordinary pip count
use the repeat variant now that the memory leak is fixed
consider batching tensorflow calls in minimax
consider just using 1-ply search and updating using the best move
general doubling algorithm as a function of payoff and equity - use analytic solution for continuous games to begin with (possibly with a 1-ply lookahead)
games: first to or number of
instead of minimax:
- alpha-beta pruning / negascout / mcts
- transposition table
- increase depth while gradually reducing width - select best k moves / use clustering to select die
  rolls
- probability of move has something to do with stability of likelihood of winning as well as the
  likelihood itself?
references:
- http://www.bkgm.com/articles/ZadehKobliska/OnOptimalDoublingInBackgammon/index.html
- https://www.cs.cornell.edu/boom/2001sp/Tsinteris/gammon.htm
- http://www.scholarpedia.org/article/User:Gerald_Tesauro/Proposed/Td-gammon
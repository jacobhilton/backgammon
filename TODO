[Td.equity] should somehow batch tensorflow calls
get variable batch size to work (check if it is currently already working)
regularization
for gammons and backgammons, pass an Outcome.t and a Player.t to equities and a (payoff : Outcome.t -> Player.t -> float) to games
the network will have to have 5 ouptut nodes, unless we copy the original idea of having an input representing whose turn it is
both calls to [Session.run] in td.ml appear to have some kind of memory leak
gammon pip count ratio (pip count to point 6 plus 1 if nothing borne off, divided by sum of that plus opponent ordinary pip count)
save more often, separate input/output
try just using 1-ply search and updating using the best move
two hidden layers? what sizes?
wrap the above options in a config of some sort
rename equity to distinguish between functions and values
report total score / gammon numbers too

instead of minimax:
- alpha-beta pruning / negascout / mcts
- transposition table
- increase depth while gradually reducing width - select best k moves / use clustering to select die
  rolls
- probability of move has something to do with stability of likelihood of winning as well as the
  likelihood itself?

http://www.bkgm.com/articles/ZadehKobliska/OnOptimalDoublingInBackgammon/index.html
https://www.cs.cornell.edu/boom/2001sp/Tsinteris/gammon.htm
http://www.scholarpedia.org/article/User:Gerald_Tesauro/Proposed/Td-gammon